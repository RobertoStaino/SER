{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, learning_curve, validation_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from utils.audio_preprocess import *\n",
    "from utils.septr import SeparableTr\n",
    "from utils.feature_exctraction_v2 import get3d_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D,MaxPooling2D, Flatten, Dropout, BatchNormalization, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#import warnings\n",
    "#if not sys.warnoptions:\n",
    "#    warnings.simplefilter(\"ignore\")\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ave maria 🙏🏻🙏🏿\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_path, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get3d_data(train, test, feats = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling our data with sklearn's Standard scaler\n",
    "# scaler = StandardScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(nfilters1=128, nfilters2=64, nfilters3=32, size_kernel=5, nstrides=1, s_pool1=5, s_pool2=5, s_pool3=5):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=nfilters1, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "    model.add(Dropout(0.2))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=s_pool1, strides = nstrides, padding = 'same'))\n",
    "\n",
    "    model.add(Conv2D(filters=nfilters2, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(MaxPooling2D(pool_size=s_pool2, strides = nstrides, padding = 'same'))\n",
    "\n",
    "    model.add(Conv2D(filters=nfilters3, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=s_pool3, strides = nstrides, padding = 'same'))\n",
    "\n",
    "    #model.add(Conv1D(filters=nfilters4, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "    model.add(Dense(units=6, activation='softmax'))\n",
    "    model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "    #rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.30, verbose=0, patience=2, min_lr=0.0000001)\n",
    "\n",
    "    #model.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test), callbacks=[rlrp])    \n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(\n",
    "    model=cnn_model, \n",
    "    verbose=0, \n",
    "    batch_size=32, \n",
    "    loss=\"categorical_crossentropy\", \n",
    "    nfilters1 = 64, \n",
    "    nfilters2 = 64, \n",
    "    nfilters3 = 64, \n",
    "    nfilters4 = 64, \n",
    "    size_kernel = 3,\n",
    "    nstrides = 1,\n",
    "    s_pool1 = 5,\n",
    "    s_pool2 = 5,\n",
    "    s_pool3 = 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'nfilters1': [256, 128, 64, 32],\n",
    "    'nfilters2': [256, 128, 64, 32],\n",
    "    'nfilters3': [256, 128, 64, 32],\n",
    "    'nfilters4': [256, 128, 64, 32],\n",
    "    'size_kernel': [3, 5],\n",
    "    'nstrides': [1, 2, 3],\n",
    "    's_pool1': [3, 5, 8],\n",
    "    's_pool2': [3, 5, 8],\n",
    "    's_pool3': [3, 5, 8]\n",
    "    #'optimizer': ['adam']\n",
    "}\n",
    "\n",
    "# Grid\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=3, verbose=3, n_jobs=3, n_iter=1000) #GridSearchCV\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "# Print\n",
    "print(\"Best hyperparameter: \", grid_result.best_params_)\n",
    "print(\"Best accuracy: \", grid_result.best_score_)\n",
    "\n",
    "best_model = grid_result.best_estimator_\n",
    "val_accuracy = best_model.score(x_test, y_test)\n",
    "print(\"Validaton accuracy: \", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/666 [====================>.........] - ETA: 2:17:16 - loss: 6.1889 - accuracy: 0.1703"
     ]
    }
   ],
   "source": [
    "cnn_model().fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CapNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, capsule_dim, routings=3, kernel_initializer='glorot_uniform', **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.capsule_dim = capsule_dim\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_dim, self.num_capsules, self.capsule_dim),\n",
    "            initializer=self.kernel_initializer,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Inputs shape: (batch_size, input_dim, input_num_capsules)\n",
    "        # W shape: (input_dim, num_capsules, capsule_dim)\n",
    "        # u_hat: (batch_size, input_dim, num_capsules, capsule_dim)\n",
    "        u_hat = tf.matmul(inputs, self.W)\n",
    "        \n",
    "        # Initialize b with zero values\n",
    "        b = tf.zeros((inputs.shape[0], input_dim, self.num_capsules))\n",
    "        \n",
    "        # Routing algorithm\n",
    "        for _ in range(self.routings):\n",
    "            c = tf.nn.softmax(b, axis=2)\n",
    "            s = tf.reduce_sum(c * u_hat, axis=1)\n",
    "            v = squash(s)\n",
    "            b += tf.reduce_sum(u_hat * v[:, tf.newaxis, :, :], axis=-1)\n",
    "\n",
    "        return v\n",
    "    \n",
    "def squash(vector):\n",
    "    vector_norm = tf.norm(vector, axis=-1, keepdims=True)\n",
    "    vector_squashed = (vector_norm / (1 + vector_norm**2)) * vector\n",
    "    return vector_squashed\n",
    "\n",
    "def cn_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(x_train.shape[1:])))  # Input shape\n",
    "    model.add(CapsuleLayer(num_capsules=10, capsule_dim=16))  # Primary Capsules\n",
    "    model.add(CapsuleLayer(num_capsules=10, capsule_dim=16))  # Digit Capsules\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"capsule_layer\" (type CapsuleLayer).\n\nin user code:\n\n    File \"/var/folders/45/bzqc3zcj52bbs_vq26k_nt8c0000gn/T/ipykernel_13066/962544458.py\", line 23, in call  *\n        u_hat = tf.matmul(inputs, self.W)\n\n    ValueError: Dimensions must be equal, but are 1 and 10 for '{{node capsule_layer/MatMul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Placeholder, capsule_layer/MatMul/ReadVariableOp)' with input shapes: [?,128,219,1], [1,10,16].\n\n\nCall arguments received by layer \"capsule_layer\" (type CapsuleLayer):\n  • inputs=tf.Tensor(shape=(None, 128, 219, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cn_model()\u001b[39m.\u001b[39mfit(x_train, y_train, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, validation_data\u001b[39m=\u001b[39m(x_test, y_test))\n",
      "Cell \u001b[0;32mIn[43], line 45\u001b[0m, in \u001b[0;36mcn_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mSequential()\n\u001b[1;32m     44\u001b[0m model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(x_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:])))  \u001b[39m# Input shape\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m model\u001b[39m.\u001b[39;49madd(CapsuleLayer(num_capsules\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, capsule_dim\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m))  \u001b[39m# Primary Capsules\u001b[39;00m\n\u001b[1;32m     46\u001b[0m model\u001b[39m.\u001b[39madd(CapsuleLayer(num_capsules\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, capsule_dim\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m))  \u001b[39m# Digit Capsules\u001b[39;00m\n\u001b[1;32m     47\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/45/bzqc3zcj52bbs_vq26k_nt8c0000gn/T/__autograph_generated_file27xod6tn.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m u_hat \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mmatmul, (ag__\u001b[39m.\u001b[39;49mld(inputs), ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mW), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     11\u001b[0m b \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mzeros, ((ag__\u001b[39m.\u001b[39mld(inputs)\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], ag__\u001b[39m.\u001b[39mld(input_dim), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnum_capsules),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m():\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"capsule_layer\" (type CapsuleLayer).\n\nin user code:\n\n    File \"/var/folders/45/bzqc3zcj52bbs_vq26k_nt8c0000gn/T/ipykernel_13066/962544458.py\", line 23, in call  *\n        u_hat = tf.matmul(inputs, self.W)\n\n    ValueError: Dimensions must be equal, but are 1 and 10 for '{{node capsule_layer/MatMul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Placeholder, capsule_layer/MatMul/ReadVariableOp)' with input shapes: [?,128,219,1], [1,10,16].\n\n\nCall arguments received by layer \"capsule_layer\" (type CapsuleLayer):\n  • inputs=tf.Tensor(shape=(None, 128, 219, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "cn_model().fit(x_train, y_train, batch_size=32, epochs=20, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SepTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
