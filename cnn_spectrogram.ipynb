{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, learning_curve, validation_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from utils.audio_preprocess import *\n",
    "from utils.septr import SeparableTr\n",
    "from utils.feature_exctraction_v2 import get3d_data, prepare_png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D,MaxPooling2D, Flatten, Dropout, BatchNormalization, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers.legacy import Adam \n",
    "\n",
    "#import warnings\n",
    "#if not sys.warnoptions:\n",
    "#    warnings.simplefilter(\"ignore\")\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ave maria üôèüèªüôèüèø\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = train_test_split(data_path, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get3d_data(data_path, feats = False, max_aug = 3, save_png=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28149 files belonging to 6 classes.\n",
      "Using 22520 files for training.\n",
      "Found 28149 files belonging to 6 classes.\n",
      "Using 5629 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Declare constants\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 6\n",
    "OUTPUT_DIR = 'Data/spectr/'\n",
    "\n",
    "# Make a dataset containing the training spectrograms\n",
    "xpng_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=OUTPUT_DIR,\n",
    "                                             shuffle=True,\n",
    "                                             label_mode='categorical',\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"training\",\n",
    "                                             seed=42)\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "xpng_test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=OUTPUT_DIR,\n",
    "                                             shuffle=True,\n",
    "                                             label_mode='categorical',\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"validation\",\n",
    "                                             seed=42)\n",
    "\n",
    "xpng_train = prepare_png(xpng_train, augment=False)\n",
    "xpng_test = prepare_png(xpng_test, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('x_train.npy', x_train)\n",
    "#np.save('y_train.npy', y_train)\n",
    "#np.save('x_test.npy', x_test)\n",
    "#np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling our data with sklearn's Standard scaler\n",
    "# scaler = StandardScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=x_train.shape[1:], batch_size=32))  # input\n",
    "    # model.add(Dense(units=256, activation='relu'))\n",
    "    # model.add(Dropout(0.25))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(units=512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(units=256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(units=128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(units=84, activation=\"relu\"))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(units=64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(units=32, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(units=16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(units=8, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(units=6, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_base_model().fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(nfilters1=32, nfilters2=64, size_kernel=5, nstrides=1, s_pool1=5, s_pool2=5, s_pool3=5):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=nfilters1, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
    "    #model.add(Dropout(0.2))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=s_pool1, strides = nstrides, padding = 'same'))\n",
    "\n",
    "    model.add(Conv2D(filters=nfilters2, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu'))\n",
    "    #model.add(Dropout(0.15))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(MaxPooling2D(pool_size=s_pool2, strides = nstrides, padding = 'same'))\n",
    "\n",
    "    #model.add(Conv1D(filters=nfilters4, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    model.add(Dropout(0.2))    \n",
    "    model.add(Dense(units=16, activation='relu'))\n",
    "    model.add(Dropout(0.1))    \n",
    "\n",
    "    model.add(Dense(units=6, activation='softmax'))\n",
    "    model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "    #rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.30, verbose=0, patience=2, min_lr=0.0000001)\n",
    "\n",
    "    #model.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test), callbacks=[rlrp])    \n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(\n",
    "    model=cnn_model, \n",
    "    verbose=0, \n",
    "    batch_size=32, \n",
    "    loss=\"categorical_crossentropy\", \n",
    "    nfilters1 = 64, \n",
    "    nfilters2 = 64, \n",
    "    nfilters3 = 64, \n",
    "    nfilters4 = 64, \n",
    "    size_kernel = 3,\n",
    "    nstrides = 1,\n",
    "    s_pool1 = 5,\n",
    "    s_pool2 = 5,\n",
    "    s_pool3 = 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'nfilters1': [256, 128, 64, 32],\n",
    "    'nfilters2': [256, 128, 64, 32],\n",
    "    'nfilters3': [256, 128, 64, 32],\n",
    "    'nfilters4': [256, 128, 64, 32],\n",
    "    'size_kernel': [3, 5],\n",
    "    'nstrides': [1, 2, 3],\n",
    "    's_pool1': [3, 5, 8],\n",
    "    's_pool2': [3, 5, 8],\n",
    "    's_pool3': [3, 5, 8]\n",
    "    #'optimizer': ['adam']\n",
    "}\n",
    "\n",
    "# Grid\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=3, verbose=3, n_jobs=3, n_iter=1000) #GridSearchCV\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "# Print\n",
    "print(\"Best hyperparameter: \", grid_result.best_params_)\n",
    "print(\"Best accuracy: \", grid_result.best_score_)\n",
    "\n",
    "best_model = grid_result.best_estimator_\n",
    "val_accuracy = best_model.score(x_test, y_test)\n",
    "print(\"Validaton accuracy: \", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 415, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0,:,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CapNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, capsule_dim, routings=3, kernel_initializer='glorot_uniform', **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.capsule_dim = capsule_dim\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_dim, self.num_capsules, self.capsule_dim),\n",
    "            initializer=self.kernel_initializer,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Inputs shape: (batch_size, input_dim, input_num_capsules)\n",
    "        # W shape: (input_dim, num_capsules, capsule_dim)\n",
    "        # u_hat: (batch_size, input_dim, num_capsules, capsule_dim)\n",
    "        u_hat = tf.matmul(inputs, self.W)\n",
    "        \n",
    "        # Initialize b with zero values\n",
    "        b = tf.zeros((inputs.shape[0], input_dim, self.num_capsules))\n",
    "        \n",
    "        # Routing algorithm\n",
    "        for _ in range(self.routings):\n",
    "            c = tf.nn.softmax(b, axis=2)\n",
    "            s = tf.reduce_sum(c * u_hat, axis=1)\n",
    "            v = squash(s)\n",
    "            b += tf.reduce_sum(u_hat * v[:, tf.newaxis, :, :], axis=-1)\n",
    "\n",
    "        return v\n",
    "    \n",
    "def squash(vector):\n",
    "    vector_norm = tf.norm(vector, axis=-1, keepdims=True)\n",
    "    vector_squashed = (vector_norm / (1 + vector_norm**2)) * vector\n",
    "    return vector_squashed\n",
    "\n",
    "def cn_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(x_train.shape[1:])))  # Input shape\n",
    "    model.add(CapsuleLayer(num_capsules=10, capsule_dim=16))  # Primary Capsules\n",
    "    model.add(CapsuleLayer(num_capsules=10, capsule_dim=16))  # Digit Capsules\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cn_model().fit(x_train, y_train, batch_size=16, epochs=20, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SepTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
