{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, learning_curve, validation_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from utils.audio_preprocess import *\n",
    "from utils.septr import SeparableTr\n",
    "from utils.feature_exctraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D,MaxPooling2D, Flatten, Dropout, BatchNormalization, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#import warnings\n",
    "#if not sys.warnoptions:\n",
    "#    warnings.simplefilter(\"ignore\")\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quante mad***e\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crema = \"CREMA-D/AudioWAV/\"\n",
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "NSIZE = len(crema_directory_list)\n",
    "SAMPLE_SIZE = min(200, NSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>CREMA-D/AudioWAV/1022_ITS_ANG_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>CREMA-D/AudioWAV/1037_ITS_ANG_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>CREMA-D/AudioWAV/1060_ITS_NEU_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>CREMA-D/AudioWAV/1075_ITS_NEU_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>CREMA-D/AudioWAV/1073_IOM_DIS_XX.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                  Path\n",
       "0    angry  CREMA-D/AudioWAV/1022_ITS_ANG_XX.wav\n",
       "1    angry  CREMA-D/AudioWAV/1037_ITS_ANG_XX.wav\n",
       "2  neutral  CREMA-D/AudioWAV/1060_ITS_NEU_XX.wav\n",
       "3  neutral  CREMA-D/AudioWAV/1075_ITS_NEU_XX.wav\n",
       "4  disgust  CREMA-D/AudioWAV/1073_IOM_DIS_XX.wav"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for file in crema_directory_list[:SAMPLE_SIZE]:\n",
    "\n",
    "    file_path.append(Crema + file)\n",
    "\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe \n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe Emotions/Path\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df['Emotions'] = pd.Categorical(Crema_df['Emotions'])\n",
    "Crema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 72)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = np.array(Crema_df.Path[Crema_df.Emotions=='fear'])[1]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "\n",
    "mel = librosa.feature.melspectrogram(y=data, sr=22050) \n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(Crema_df, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre new axis: (128, 123)\n",
      "post new axis: (1, 128, 123)\n",
      "pre new axis: (128, 114)\n",
      "post new axis: (1, 128, 114)\n",
      "pre new axis: (128, 108)\n",
      "post new axis: (1, 128, 108)\n",
      "pre new axis: (128, 133)\n",
      "post new axis: (1, 128, 133)\n",
      "pre new axis: (128, 108)\n",
      "post new axis: (1, 128, 108)\n",
      "pre new axis: (128, 88)\n",
      "post new axis: (1, 128, 88)\n",
      "pre new axis: (128, 99)\n",
      "post new axis: (1, 128, 99)\n",
      "pre new axis: (128, 117)\n",
      "post new axis: (1, 128, 117)\n",
      "pre new axis: (128, 108)\n",
      "post new axis: (1, 128, 108)\n",
      "pre new axis: (128, 160)\n",
      "post new axis: (1, 128, 160)\n",
      "pre new axis: (128, 105)\n",
      "post new axis: (1, 128, 105)\n",
      "pre new axis: (128, 103)\n",
      "post new axis: (1, 128, 103)\n",
      "pre new axis: (128, 124)\n",
      "post new axis: (1, 128, 124)\n",
      "pre new axis: (128, 101)\n",
      "post new axis: (1, 128, 101)\n",
      "pre new axis: (128, 110)\n",
      "post new axis: (1, 128, 110)\n",
      "pre new axis: (128, 104)\n",
      "post new axis: (1, 128, 104)\n",
      "pre new axis: (128, 134)\n",
      "post new axis: (1, 128, 134)\n",
      "pre new axis: (128, 150)\n",
      "post new axis: (1, 128, 150)\n",
      "pre new axis: (128, 115)\n",
      "post new axis: (1, 128, 115)\n",
      "pre new axis: (128, 98)\n",
      "post new axis: (1, 128, 98)\n",
      "pre new axis: (128, 92)\n",
      "post new axis: (1, 128, 92)\n",
      "pre new axis: (128, 126)\n",
      "post new axis: (1, 128, 126)\n",
      "pre new axis: (128, 90)\n",
      "post new axis: (1, 128, 90)\n",
      "pre new axis: (128, 117)\n",
      "post new axis: (1, 128, 117)\n",
      "pre new axis: (128, 141)\n",
      "post new axis: (1, 128, 141)\n",
      "pre new axis: (128, 105)\n",
      "post new axis: (1, 128, 105)\n",
      "pre new axis: (128, 117)\n",
      "post new axis: (1, 128, 117)\n",
      "pre new axis: (128, 133)\n",
      "post new axis: (1, 128, 133)\n",
      "pre new axis: (128, 107)\n",
      "post new axis: (1, 128, 107)\n",
      "pre new axis: (128, 120)\n",
      "post new axis: (1, 128, 120)\n",
      "pre new axis: (128, 104)\n",
      "post new axis: (1, 128, 104)\n",
      "pre new axis: (128, 104)\n",
      "post new axis: (1, 128, 104)\n",
      "pre new axis: (128, 94)\n",
      "post new axis: (1, 128, 94)\n",
      "pre new axis: (128, 126)\n",
      "post new axis: (1, 128, 126)\n",
      "pre new axis: (128, 114)\n",
      "post new axis: (1, 128, 114)\n",
      "pre new axis: (128, 130)\n",
      "post new axis: (1, 128, 130)\n",
      "pre new axis: (128, 74)\n",
      "post new axis: (1, 128, 74)\n",
      "pre new axis: (128, 107)\n",
      "post new axis: (1, 128, 107)\n",
      "pre new axis: (128, 130)\n",
      "post new axis: (1, 128, 130)\n",
      "pre new axis: (128, 80)\n",
      "post new axis: (1, 128, 80)\n",
      "pre new axis: (128, 104)\n",
      "post new axis: (1, 128, 104)\n",
      "pre new axis: (128, 133)\n",
      "post new axis: (1, 128, 133)\n",
      "pre new axis: (128, 95)\n",
      "post new axis: (1, 128, 95)\n",
      "pre new axis: (128, 100)\n",
      "post new axis: (1, 128, 100)\n",
      "pre new axis: (128, 115)\n",
      "post new axis: (1, 128, 115)\n",
      "pre new axis: (128, 118)\n",
      "post new axis: (1, 128, 118)\n",
      "pre new axis: (128, 117)\n",
      "post new axis: (1, 128, 117)\n",
      "pre new axis: (128, 105)\n",
      "post new axis: (1, 128, 105)\n",
      "pre new axis: (128, 92)\n",
      "post new axis: (1, 128, 92)\n",
      "pre new axis: (128, 126)\n",
      "post new axis: (1, 128, 126)\n",
      "pre new axis: (128, 124)\n",
      "post new axis: (1, 128, 124)\n",
      "pre new axis: (128, 98)\n",
      "post new axis: (1, 128, 98)\n",
      "pre new axis: (128, 110)\n",
      "post new axis: (1, 128, 110)\n",
      "pre new axis: (128, 147)\n",
      "post new axis: (1, 128, 147)\n",
      "pre new axis: (128, 91)\n",
      "post new axis: (1, 128, 91)\n",
      "pre new axis: (128, 136)\n",
      "post new axis: (1, 128, 136)\n",
      "pre new axis: (128, 72)\n",
      "post new axis: (1, 128, 72)\n",
      "pre new axis: (128, 100)\n",
      "post new axis: (1, 128, 100)\n",
      "pre new axis: (128, 97)\n",
      "post new axis: (1, 128, 97)\n",
      "pre new axis: (128, 81)\n",
      "post new axis: (1, 128, 81)\n",
      "pre new axis: (128, 105)\n",
      "post new axis: (1, 128, 105)\n",
      "pre new axis: (128, 105)\n",
      "post new axis: (1, 128, 105)\n",
      "pre new axis: (128, 92)\n",
      "post new axis: (1, 128, 92)\n",
      "pre new axis: (128, 130)\n",
      "post new axis: (1, 128, 130)\n",
      "pre new axis: (128, 90)\n",
      "post new axis: (1, 128, 90)\n",
      "pre new axis: (128, 88)\n",
      "post new axis: (1, 128, 88)\n",
      "pre new axis: (128, 150)\n",
      "post new axis: (1, 128, 150)\n",
      "pre new axis: (128, 114)\n",
      "post new axis: (1, 128, 114)\n",
      "pre new axis: (128, 84)\n",
      "post new axis: (1, 128, 84)\n",
      "pre new axis: (128, 82)\n",
      "post new axis: (1, 128, 82)\n",
      "pre new axis: (128, 117)\n",
      "post new axis: (1, 128, 117)\n",
      "pre new axis: (128, 111)\n",
      "post new axis: (1, 128, 111)\n",
      "pre new axis: (128, 127)\n",
      "post new axis: (1, 128, 127)\n",
      "pre new axis: (128, 108)\n",
      "post new axis: (1, 128, 108)\n",
      "pre new axis: (128, 120)\n",
      "post new axis: (1, 128, 120)\n",
      "pre new axis: (128, 105)\n",
      "post new axis: (1, 128, 105)\n",
      "pre new axis: (128, 114)\n",
      "post new axis: (1, 128, 114)\n",
      "pre new axis: (128, 137)\n",
      "post new axis: (1, 128, 137)\n",
      "pre new axis: (128, 156)\n",
      "post new axis: (1, 128, 156)\n",
      "pre new axis: (128, 133)\n",
      "post new axis: (1, 128, 133)\n",
      "pre new axis: (128, 94)\n",
      "post new axis: (1, 128, 94)\n",
      "pre new axis: (128, 146)\n",
      "post new axis: (1, 128, 146)\n",
      "pre new axis: (128, 123)\n",
      "post new axis: (1, 128, 123)\n",
      "pre new axis: (128, 115)\n",
      "post new axis: (1, 128, 115)\n",
      "pre new axis: (128, 104)\n",
      "post new axis: (1, 128, 104)\n",
      "pre new axis: (128, 98)\n",
      "post new axis: (1, 128, 98)\n",
      "pre new axis: (128, 105)\n",
      "post new axis: (1, 128, 105)\n",
      "pre new axis: (128, 71)\n",
      "post new axis: (1, 128, 71)\n",
      "pre new axis: (128, 124)\n",
      "post new axis: (1, 128, 124)\n",
      "pre new axis: (128, 84)\n",
      "post new axis: (1, 128, 84)\n",
      "pre new axis: (128, 104)\n",
      "post new axis: (1, 128, 104)\n",
      "pre new axis: (128, 108)\n",
      "post new axis: (1, 128, 108)\n",
      "pre new axis: (128, 101)\n",
      "post new axis: (1, 128, 101)\n",
      "pre new axis: (128, 114)\n",
      "post new axis: (1, 128, 114)\n",
      "pre new axis: (128, 67)\n",
      "post new axis: (1, 128, 67)\n",
      "pre new axis: (128, 120)\n",
      "post new axis: (1, 128, 120)\n",
      "pre new axis: (128, 159)\n",
      "post new axis: (1, 128, 159)\n",
      "pre new axis: (128, 140)\n",
      "post new axis: (1, 128, 140)\n",
      "pre new axis: (128, 91)\n",
      "post new axis: (1, 128, 91)\n",
      "pre new axis: (128, 115)\n",
      "post new axis: (1, 128, 115)\n",
      "pre new axis: (128, 118)\n",
      "post new axis: (1, 128, 118)\n",
      "pre new axis: (128, 154)\n",
      "post new axis: (1, 128, 154)\n",
      "pre new axis: (128, 92)\n",
      "post new axis: (1, 128, 92)\n",
      "pre new axis: (128, 95)\n",
      "post new axis: (1, 128, 95)\n",
      "pre new axis: (128, 124)\n",
      "post new axis: (1, 128, 124)\n",
      "pre new axis: (128, 114)\n",
      "post new axis: (1, 128, 114)\n",
      "pre new axis: (128, 137)\n",
      "post new axis: (1, 128, 137)\n",
      "pre new axis: (128, 130)\n",
      "post new axis: (1, 128, 130)\n",
      "pre new axis: (128, 92)\n",
      "post new axis: (1, 128, 92)\n",
      "pre new axis: (128, 111)\n",
      "post new axis: (1, 128, 111)\n",
      "pre new axis: (128, 107)\n",
      "post new axis: (1, 128, 107)\n",
      "pre new axis: (128, 88)\n",
      "post new axis: (1, 128, 88)\n",
      "pre new axis: (128, 107)\n",
      "post new axis: (1, 128, 107)\n",
      "pre new axis: (128, 126)\n",
      "post new axis: (1, 128, 126)\n",
      "pre new axis: (128, 118)\n",
      "post new axis: (1, 128, 118)\n",
      "pre new axis: (128, 117)\n",
      "post new axis: (1, 128, 117)\n",
      "pre new axis: (128, 134)\n",
      "post new axis: (1, 128, 134)\n",
      "pre new axis: (128, 114)\n",
      "post new axis: (1, 128, 114)\n",
      "pre new axis: (128, 124)\n",
      "post new axis: (1, 128, 124)\n",
      "pre new axis: (128, 127)\n",
      "post new axis: (1, 128, 127)\n",
      "pre new axis: (128, 113)\n",
      "post new axis: (1, 128, 113)\n",
      "pre new axis: (128, 99)\n",
      "post new axis: (1, 128, 99)\n",
      "pre new axis: (128, 101)\n",
      "post new axis: (1, 128, 101)\n",
      "pre new axis: (128, 104)\n",
      "post new axis: (1, 128, 104)\n",
      "pre new axis: (128, 138)\n",
      "post new axis: (1, 128, 138)\n",
      "pre new axis: (128, 120)\n",
      "post new axis: (1, 128, 120)\n",
      "pre new axis: (128, 100)\n",
      "post new axis: (1, 128, 100)\n",
      "pre new axis: (128, 120)\n",
      "post new axis: (1, 128, 120)\n",
      "pre new axis: (128, 154)\n",
      "post new axis: (1, 128, 154)\n",
      "pre new axis: (128, 114)\n",
      "post new axis: (1, 128, 114)\n",
      "pre new axis: (128, 130)\n",
      "post new axis: (1, 128, 130)\n",
      "pre new axis: (128, 130)\n",
      "post new axis: (1, 128, 130)\n",
      "pre new axis: (128, 146)\n",
      "post new axis: (1, 128, 146)\n",
      "pre new axis: (128, 88)\n",
      "post new axis: (1, 128, 88)\n",
      "pre new axis: (128, 94)\n",
      "post new axis: (1, 128, 94)\n",
      "pre new axis: (128, 100)\n",
      "post new axis: (1, 128, 100)\n",
      "pre new axis: (128, 133)\n",
      "post new axis: (1, 128, 133)\n",
      "pre new axis: (128, 94)\n",
      "post new axis: (1, 128, 94)\n",
      "pre new axis: (128, 137)\n",
      "post new axis: (1, 128, 137)\n",
      "pre new axis: (128, 100)\n",
      "post new axis: (1, 128, 100)\n",
      "pre new axis: (128, 105)\n",
      "post new axis: (1, 128, 105)\n",
      "pre new axis: (128, 137)\n",
      "post new axis: (1, 128, 137)\n",
      "pre new axis: (128, 113)\n",
      "post new axis: (1, 128, 113)\n",
      "pre new axis: (128, 111)\n",
      "post new axis: (1, 128, 111)\n",
      "pre new axis: (128, 85)\n",
      "post new axis: (1, 128, 85)\n",
      "pre new axis: (128, 130)\n",
      "post new axis: (1, 128, 130)\n",
      "pre new axis: (128, 99)\n",
      "post new axis: (1, 128, 99)\n",
      "pre new axis: (128, 130)\n",
      "post new axis: (1, 128, 130)\n",
      "pre new axis: (128, 118)\n",
      "post new axis: (1, 128, 118)\n",
      "pre new axis: (128, 136)\n",
      "post new axis: (1, 128, 136)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = [], []\n",
    "x_test, y_test = [], []\n",
    "\n",
    "# TRAIN + Augmentation\n",
    "for path, emotion in zip(train.Path, train.Emotions):\n",
    "    feature = get_features(path, synth = False, spect = True, debug = False)\n",
    "    for ele in feature:\n",
    "        x_train.append(ele)\n",
    "        y_train.append(emotion)\n",
    "\n",
    "# TEST\n",
    "for path, emotion in zip(test.Path, test.Emotions):\n",
    "    feature = get_features(path, synth = False, spect = True)\n",
    "    for ele in feature:\n",
    "        x_test.append(ele)\n",
    "        y_test.append(emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rows, max_cols = 0,0\n",
    "for atens in [x_train, x_test]:\n",
    "    # Find the maximum number of rows and columns among all arrays\n",
    "    max_rows = max(max_rows, max(a.shape[0] for a in atens))\n",
    "    max_cols = max(max_cols, max(a.shape[1] for a in atens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for atens in [x_train, x_test]:\n",
    "    for i, a in enumerate(atens):\n",
    "        # Pad the smaller arrays with zeros to match the maximum shape\n",
    "        atens[i] = np.pad(a, ((0, max_rows - a.shape[0]), (0, max_cols - a.shape[1])), 'constant')\n",
    "\n",
    "# Stack the padded arrays using numpy.stack()\n",
    "x_train = np.stack(x_train)\n",
    "x_test = np.stack(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "y_train = encoder.fit_transform(np.array(y_train).reshape(-1,1)).toarray()\n",
    "y_test = encoder.fit_transform(np.array(y_test).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making our data compatible to model.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test =  np.expand_dims(x_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5581, 216, 128, 1), (5581, 6), (1861, 216, 128, 1), (1861, 6))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling our data with sklearn's Standard scaler\n",
    "# scaler = StandardScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(nfilters1=128, nfilters2=64, nfilters3=32, size_kernel=5, nstrides=1, size_pool=5):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=nfilters1, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "    model.add(Dropout(0.2))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Conv2D(filters=nfilters2, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(MaxPooling2D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(filters=nfilters3, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    #model.add(Conv1D(filters=nfilters4, kernel_size=size_kernel, strides=nstrides, padding='same', activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "    model.add(Dense(units=6, activation='softmax'))\n",
    "    model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "    #rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.30, verbose=0, patience=2, min_lr=0.0000001)\n",
    "\n",
    "    #model.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test), callbacks=[rlrp])    \n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "175/175 [==============================] - 488s 3s/step - loss: 2.0476 - accuracy: 0.1980 - val_loss: 1.8047 - val_accuracy: 0.1671\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 1231s 7s/step - loss: 1.8003 - accuracy: 0.1706 - val_loss: 1.7956 - val_accuracy: 0.1698\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 486s 3s/step - loss: 1.7033 - accuracy: 0.2731 - val_loss: 2.0244 - val_accuracy: 0.1983\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 885s 5s/step - loss: 1.6095 - accuracy: 0.3290 - val_loss: 1.6575 - val_accuracy: 0.2563\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 1312s 8s/step - loss: 1.5765 - accuracy: 0.3395 - val_loss: 1.6323 - val_accuracy: 0.2832\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 2044s 12s/step - loss: 1.5618 - accuracy: 0.3471 - val_loss: 2.6254 - val_accuracy: 0.1730\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 4898s 28s/step - loss: 1.5365 - accuracy: 0.3489 - val_loss: 1.6078 - val_accuracy: 0.2886\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 8009s 46s/step - loss: 1.5238 - accuracy: 0.3460 - val_loss: 3.0319 - val_accuracy: 0.1628\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 7826s 45s/step - loss: 1.5174 - accuracy: 0.3515 - val_loss: 1.6731 - val_accuracy: 0.3036\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 6946s 40s/step - loss: 1.5148 - accuracy: 0.3499 - val_loss: 2.1119 - val_accuracy: 0.2402\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 7438s 43s/step - loss: 1.5057 - accuracy: 0.3596 - val_loss: 7.8975 - val_accuracy: 0.2080\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 3325s 19s/step - loss: 1.4979 - accuracy: 0.3560 - val_loss: 9.4824 - val_accuracy: 0.1779\n",
      "Epoch 13/20\n",
      "157/175 [=========================>....] - ETA: 40s - loss: 1.4881 - accuracy: 0.3589"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cnn_model()\u001b[39m.\u001b[39;49mfit(x_train, y_train, validation_data\u001b[39m=\u001b[39;49m(x_test, y_test), epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Università/SER/.venv3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_model().fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SepTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
